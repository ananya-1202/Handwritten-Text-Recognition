{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import os \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [] #array to store training and testing images\n",
    "Ytrain = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing \n",
    "#Training the model with data from task 1 to use it later as pre-trainined model for transfer learning for MNIST dataset. \n",
    "#Here the model is trained for only 10 classes 0-9\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "classes = 0 \n",
    "add = r\"trainPart1\\train\\g\" #address of training data\n",
    "original_data = 0\n",
    "for data in os.listdir(add[:-2]):\n",
    "    p = add[:-1] + data + \"\\g\"\n",
    "    original_data += len(os.listdir(p[:-1]))\n",
    "    if classes < 10 : #first 10 classes in the training data is 0-9\n",
    "        for d in os.listdir(p[:-1]):  \n",
    "            img = cv2.imread(p[:-1] + d) \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "            #To reduce the size of the input image for the model and removing the unwanted white space in the image. \n",
    "            black = np.column_stack(np.where(img == 0))\n",
    "            x_values=[i[0] for i in black]\n",
    "            y_values=[i[1] for i in black]\n",
    "            xMin , xMax , yMin , yMax = min(x_values) , max(x_values) , min(y_values) , max(y_values)\n",
    "            img = img[xMin - 10 : xMax + 10 , yMin - 10 : yMax + 10 ] #Creating an ROI around the text and leaving \n",
    "            #and leaving a margin of 10px\n",
    "            img = cv2.resize(img, (28,28)) \n",
    "            img = img.reshape(1,img.shape[0],img.shape[1],1)\n",
    "            datagen = ImageDataGenerator(rotation_range=90) #Image Augmentation to icrease the number of training data \n",
    "            it = datagen.flow(img, batch_size=1)\n",
    "            for i in range(4):\n",
    "                batch = it.next()\n",
    "                image = batch[0].astype('uint8')\n",
    "                Xtrain.append(image)\n",
    "                Ytrain.append(classes)\n",
    "        classes += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Classes : 10\n",
      "Size of Training data before data augmentation: 2480\n",
      "Size of Training data after data augmentation: 1600\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Classes : \" + str(classes))\n",
    "print(\"Size of Training data before data augmentation: \" + str(original_data))\n",
    "print(\"Size of Training data after data augmentation: \" + str(len(Xtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prepration\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "Xtrain , Ytrain = shuffle(Xtrain , Ytrain) #Shuffling the data to create independent change in the data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42) #Spliting the data \n",
    "#into training and valditation data \n",
    "\n",
    "img_row = x_train[0].shape[0]\n",
    "img_col = x_train[0].shape[1]\n",
    "\n",
    "x_train  = np.array(x_train)\n",
    "x_test  = np.array(x_test)\n",
    "\n",
    "x_train=x_train.reshape(x_train.shape[0], img_row, img_col, 1)\n",
    "x_test=x_test.reshape(x_test.shape[0], img_row, img_col, 1)\n",
    "\n",
    "input_shape = (img_row, img_col, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255 #data Normalization\n",
    "x_test /= 255\n",
    "\n",
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "num_classes = y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture:\n",
    "#Model for transfer learning \n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , MaxPooling2D , Flatten,Dropout , BatchNormalization , Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation = 'relu'))\n",
    "model.add(BatchNormalization()) #To re-center and re-scale the layer\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3)) #To reduce over-fitting \n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(classes,activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=SGD(0.01),metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = r\"model2_1.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 90,570\n",
      "Trainable params: 90,250\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 3s 63ms/step - loss: 3.2729 - accuracy: 0.1261 - val_loss: 2.3009 - val_accuracy: 0.1193\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11932, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 2.0412 - accuracy: 0.2924 - val_loss: 2.3012 - val_accuracy: 0.0947\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.11932\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 1.8017 - accuracy: 0.3760 - val_loss: 2.2997 - val_accuracy: 0.1080\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.11932\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 1.6077 - accuracy: 0.4626 - val_loss: 2.2991 - val_accuracy: 0.1307\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.11932 to 0.13068, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.3821 - accuracy: 0.5154 - val_loss: 2.2984 - val_accuracy: 0.1212\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.13068\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 1.3475 - accuracy: 0.5283 - val_loss: 2.2865 - val_accuracy: 0.1629\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.13068 to 0.16288, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 1.2516 - accuracy: 0.5607 - val_loss: 2.2650 - val_accuracy: 0.1742\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.16288 to 0.17424, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 1.0678 - accuracy: 0.6166 - val_loss: 2.2366 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.17424 to 0.18750, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.9985 - accuracy: 0.6396 - val_loss: 2.1825 - val_accuracy: 0.2178\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.18750 to 0.21780, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.9782 - accuracy: 0.6596 - val_loss: 2.0973 - val_accuracy: 0.2462\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.21780 to 0.24621, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.8805 - accuracy: 0.6939 - val_loss: 2.0120 - val_accuracy: 0.2727\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.24621 to 0.27273, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.7848 - accuracy: 0.7293 - val_loss: 1.8626 - val_accuracy: 0.3731\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.27273 to 0.37311, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.7405 - accuracy: 0.7561 - val_loss: 1.7332 - val_accuracy: 0.4602\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.37311 to 0.46023, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.7329 - accuracy: 0.7371 - val_loss: 1.4463 - val_accuracy: 0.5739\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.46023 to 0.57386, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.6866 - accuracy: 0.7722 - val_loss: 1.1316 - val_accuracy: 0.7159\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.57386 to 0.71591, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.7032 - accuracy: 0.7511 - val_loss: 0.9586 - val_accuracy: 0.7614\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.71591 to 0.76136, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.6350 - accuracy: 0.7675 - val_loss: 0.6851 - val_accuracy: 0.8390\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.76136 to 0.83902, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.5596 - accuracy: 0.8022 - val_loss: 0.6108 - val_accuracy: 0.8428\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.83902 to 0.84280, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.5409 - accuracy: 0.7932 - val_loss: 0.5710 - val_accuracy: 0.8693\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.84280 to 0.86932, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.5433 - accuracy: 0.8063 - val_loss: 0.4589 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.86932 to 0.87689, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.4741 - accuracy: 0.8251 - val_loss: 0.4510 - val_accuracy: 0.8902\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.87689 to 0.89015, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.4190 - accuracy: 0.8712 - val_loss: 0.4502 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.89015\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.4489 - accuracy: 0.8562 - val_loss: 0.3904 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.89015 to 0.89205, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.4433 - accuracy: 0.8459 - val_loss: 0.3517 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.89205\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.4355 - accuracy: 0.8588 - val_loss: 0.3410 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.89205 to 0.89583, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.4141 - accuracy: 0.8686 - val_loss: 0.3540 - val_accuracy: 0.8939\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.89583\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.4631 - accuracy: 0.8371 - val_loss: 0.3136 - val_accuracy: 0.9053\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.89583 to 0.90530, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.4418 - accuracy: 0.8323 - val_loss: 0.3128 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.90530\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.3773 - accuracy: 0.8756 - val_loss: 0.3570 - val_accuracy: 0.8864\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90530\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.4023 - accuracy: 0.8661 - val_loss: 0.3495 - val_accuracy: 0.8807\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.90530\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.3926 - accuracy: 0.8706 - val_loss: 0.2757 - val_accuracy: 0.9129\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.90530 to 0.91288, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.3144 - accuracy: 0.8889 - val_loss: 0.2665 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.91288 to 0.91856, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.3303 - accuracy: 0.8896 - val_loss: 0.2602 - val_accuracy: 0.9091\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.91856\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.3044 - accuracy: 0.9053 - val_loss: 0.2501 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.91856 to 0.92045, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.3463 - accuracy: 0.8645 - val_loss: 0.2428 - val_accuracy: 0.9261\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.92045 to 0.92614, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.3503 - accuracy: 0.8762 - val_loss: 0.2408 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00036: val_accuracy improved from 0.92614 to 0.93371, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.2928 - accuracy: 0.9025 - val_loss: 0.2393 - val_accuracy: 0.9242\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.93371\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.2890 - accuracy: 0.8938 - val_loss: 0.2398 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.93371\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.2747 - accuracy: 0.9001 - val_loss: 0.2593 - val_accuracy: 0.9280\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.93371\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.2827 - accuracy: 0.8908 - val_loss: 0.2657 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.93371\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.2527 - accuracy: 0.9128 - val_loss: 0.2356 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.93371\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.2669 - accuracy: 0.9145 - val_loss: 0.2279 - val_accuracy: 0.9356\n",
      "\n",
      "Epoch 00042: val_accuracy improved from 0.93371 to 0.93561, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 0.2471 - accuracy: 0.9161 - val_loss: 0.2374 - val_accuracy: 0.9356\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.93561\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.2286 - accuracy: 0.9165 - val_loss: 0.2068 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.93561\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.2562 - accuracy: 0.9260 - val_loss: 0.2047 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00045: val_accuracy improved from 0.93561 to 0.93939, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.2031 - accuracy: 0.9336 - val_loss: 0.2146 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.93939\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.1906 - accuracy: 0.9280 - val_loss: 0.2393 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.93939\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.2299 - accuracy: 0.9233 - val_loss: 0.2047 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.93939\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.1880 - accuracy: 0.9361 - val_loss: 0.2223 - val_accuracy: 0.9299\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.93939\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.1975 - accuracy: 0.9263 - val_loss: 0.2843 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.93939\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1931 - accuracy: 0.9416 - val_loss: 0.2004 - val_accuracy: 0.9432\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.93939 to 0.94318, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.2251 - accuracy: 0.9113 - val_loss: 0.1904 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.94318\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.2134 - accuracy: 0.9282 - val_loss: 0.1909 - val_accuracy: 0.9413\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.94318\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.1640 - accuracy: 0.9528 - val_loss: 0.2018 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.94318\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 0.1546 - accuracy: 0.9448 - val_loss: 0.1835 - val_accuracy: 0.9451\n",
      "\n",
      "Epoch 00055: val_accuracy improved from 0.94318 to 0.94508, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 0.1824 - accuracy: 0.9342 - val_loss: 0.1942 - val_accuracy: 0.9337\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.94508\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 0.1313 - accuracy: 0.9594 - val_loss: 0.1935 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.94508\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.1811 - accuracy: 0.9380 - val_loss: 0.1898 - val_accuracy: 0.9413\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.94508\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.2011 - accuracy: 0.9311 - val_loss: 0.2092 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.94508\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.1565 - accuracy: 0.9480 - val_loss: 0.1646 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00060: val_accuracy improved from 0.94508 to 0.94697, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.1687 - accuracy: 0.9489 - val_loss: 0.1752 - val_accuracy: 0.9451\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.94697\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 0.1363 - accuracy: 0.9520 - val_loss: 0.1525 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00062: val_accuracy improved from 0.94697 to 0.95076, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.1221 - accuracy: 0.9653 - val_loss: 0.1482 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.95076\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1535 - accuracy: 0.9480 - val_loss: 0.1584 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.95076\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 0.1220 - accuracy: 0.9564 - val_loss: 0.1535 - val_accuracy: 0.9527\n",
      "\n",
      "Epoch 00065: val_accuracy improved from 0.95076 to 0.95265, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.1338 - accuracy: 0.9578 - val_loss: 0.1557 - val_accuracy: 0.9356\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.95265\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.1659 - accuracy: 0.9423 - val_loss: 0.1773 - val_accuracy: 0.9432\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.95265\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1523 - accuracy: 0.9487 - val_loss: 0.1650 - val_accuracy: 0.9451\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.95265\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.1103 - accuracy: 0.9624 - val_loss: 0.1710 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.95265\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 0.1506 - accuracy: 0.9435 - val_loss: 0.1566 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.95265\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 0.1254 - accuracy: 0.9611 - val_loss: 0.1743 - val_accuracy: 0.9413\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.95265\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 0.1558 - accuracy: 0.9505 - val_loss: 0.1996 - val_accuracy: 0.9318\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.95265\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1363 - accuracy: 0.9538 - val_loss: 0.1723 - val_accuracy: 0.9432\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.95265\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.1051 - accuracy: 0.9632 - val_loss: 0.1668 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.95265\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 0.1251 - accuracy: 0.9596 - val_loss: 0.1535 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.95265\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 0.0898 - accuracy: 0.9735 - val_loss: 0.1454 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.95265\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.1284 - accuracy: 0.9598 - val_loss: 0.1461 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.95265\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1108 - accuracy: 0.9632 - val_loss: 0.1638 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.95265\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1357 - accuracy: 0.9539 - val_loss: 0.1400 - val_accuracy: 0.9470\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.95265\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.0957 - accuracy: 0.9677 - val_loss: 0.1656 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.95265\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.1314 - accuracy: 0.9588 - val_loss: 0.1598 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.95265\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.0878 - accuracy: 0.9765 - val_loss: 0.1543 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.95265\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.0895 - accuracy: 0.9662 - val_loss: 0.1504 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.95265\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.1051 - accuracy: 0.9670 - val_loss: 0.1620 - val_accuracy: 0.9413\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.95265\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.0945 - accuracy: 0.9678 - val_loss: 0.1705 - val_accuracy: 0.9394\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.95265\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.1142 - accuracy: 0.9671 - val_loss: 0.1546 - val_accuracy: 0.9413\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.95265\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.0939 - accuracy: 0.9755 - val_loss: 0.1439 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.95265\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 1s 40ms/step - loss: 0.0990 - accuracy: 0.9742 - val_loss: 0.1452 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.95265\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.0937 - accuracy: 0.9601 - val_loss: 0.1632 - val_accuracy: 0.9432\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.95265\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.1039 - accuracy: 0.9647 - val_loss: 0.1428 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.95265\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.1010 - accuracy: 0.9626 - val_loss: 0.1376 - val_accuracy: 0.9489\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.95265\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 0.1074 - accuracy: 0.9641 - val_loss: 0.1403 - val_accuracy: 0.9621\n",
      "\n",
      "Epoch 00092: val_accuracy improved from 0.95265 to 0.96212, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_1.h5\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.0950 - accuracy: 0.9667 - val_loss: 0.1356 - val_accuracy: 0.9527\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.96212\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 0.0785 - accuracy: 0.9748 - val_loss: 0.1461 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.96212\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.0823 - accuracy: 0.9758 - val_loss: 0.1394 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.96212\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.1214 - accuracy: 0.9546 - val_loss: 0.1323 - val_accuracy: 0.9564\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.96212\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.0967 - accuracy: 0.9608 - val_loss: 0.1335 - val_accuracy: 0.9583\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.96212\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 1s 38ms/step - loss: 0.1105 - accuracy: 0.9673 - val_loss: 0.1355 - val_accuracy: 0.9432\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.96212\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 0.0769 - accuracy: 0.9726 - val_loss: 0.1369 - val_accuracy: 0.9451\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.96212\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 0.0754 - accuracy: 0.9705 - val_loss: 0.1415 - val_accuracy: 0.9545\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.96212\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,batch_size=32,epochs=100, callbacks=callbacks_list ,verbose=1 , validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model2_1.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720149040222168"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning using the pre-trainined model from model trained \n",
    "\n",
    "from keras.models import load_model\n",
    "model2_2 = load_model(r\"model2_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input , Dense\n",
    "\n",
    "ll = model2_2.layers[16].output\n",
    "ll = Dense(10,activation=\"softmax\")(ll)\n",
    "#input_data = Input(shape=(28, 28, 1), name='input')\n",
    "model2_22 = Model(inputs=model2_2.input,outputs=ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_input (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 90,570\n",
      "Trainable params: 90,250\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2_22.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = r\"\\model2_21.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist \n",
    "#loading the MNIST dataset \n",
    "(x_train, y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "img_row = x_train[0].shape[0]\n",
    "img_col = x_train[0].shape[1]\n",
    "\n",
    "x_train=x_train.reshape(x_train.shape[0], img_row, img_col, 1)\n",
    "x_test=x_test.reshape(x_test.shape[0], img_row, img_col, 1)\n",
    "\n",
    "input_shape = (img_row, img_col, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "from keras.utils import np_utils \n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 54s 28ms/step - loss: 0.6948 - accuracy: 0.7894 - val_loss: 0.0547 - val_accuracy: 0.9837\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.98370, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.1489 - accuracy: 0.9535 - val_loss: 0.0456 - val_accuracy: 0.9852\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.98370 to 0.98520, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.1156 - accuracy: 0.9640 - val_loss: 0.0364 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.98520 to 0.98820, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0912 - accuracy: 0.9730 - val_loss: 0.0329 - val_accuracy: 0.9890\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.98820 to 0.98900, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0287 - val_accuracy: 0.9896\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.98900 to 0.98960, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0758 - accuracy: 0.9769 - val_loss: 0.0239 - val_accuracy: 0.9912\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.98960 to 0.99120, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0671 - accuracy: 0.9803 - val_loss: 0.0277 - val_accuracy: 0.9900\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.99120\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 73s 39ms/step - loss: 0.0639 - accuracy: 0.9806 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.99120 to 0.99220, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.0237 - val_accuracy: 0.9914\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.99220\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 61s 32ms/step - loss: 0.0551 - accuracy: 0.9829 - val_loss: 0.0212 - val_accuracy: 0.9929\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.99220 to 0.99290, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_21.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "model2_22.compile(loss='categorical_crossentropy',optimizer=SGD(0.01),metrics=['accuracy'])\n",
    "history1 = model2_22.fit(x_train,y_train,batch_size=32,epochs=10, callbacks=callbacks_list ,verbose=1 , validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture:\n",
    "#training from scratch \n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , MaxPooling2D , Flatten,Dropout , BatchNormalization , Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model2_3 = Sequential()\n",
    "\n",
    "model2_3.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=input_shape))\n",
    "model2_3.add(BatchNormalization())\n",
    "model2_3.add(Activation('relu'))\n",
    "model2_3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2_3.add(Conv2D(64,kernel_size=(3,3),activation = 'relu'))\n",
    "model2_3.add(BatchNormalization()) #To re-center and re-scale the layer\n",
    "model2_3.add(Activation('relu'))\n",
    "model2_3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2_3.add(Dropout(0.3)) #To reduce over-fitting \n",
    "\n",
    "model2_3.add(Conv2D(64,kernel_size=(3,3),activation = 'relu'))\n",
    "model2_3.add(BatchNormalization())\n",
    "model2_3.add(Activation('relu'))\n",
    "model2_3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2_3.add(Dropout(0.3))\n",
    "\n",
    "model2_3.add(Flatten())\n",
    "model2_3.add(Dense(128,activation='relu'))\n",
    "model2_3.add(Dropout(0.2))\n",
    "\n",
    "model2_3.add(Dense(classes,activation = 'softmax'))\n",
    "\n",
    "model2_3.compile(loss='categorical_crossentropy',optimizer=SGD(0.01),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 90,570\n",
      "Trainable params: 90,250\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = r\"\\model2_3.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 62s 32ms/step - loss: 0.7630 - accuracy: 0.7545 - val_loss: 0.0591 - val_accuracy: 0.9819\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.98190, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 103s 55ms/step - loss: 0.1522 - accuracy: 0.9526 - val_loss: 0.0454 - val_accuracy: 0.9860\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.98190 to 0.98600, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 109s 58ms/step - loss: 0.1054 - accuracy: 0.9676 - val_loss: 0.0366 - val_accuracy: 0.9878\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.98600 to 0.98780, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 116s 62ms/step - loss: 0.0894 - accuracy: 0.9725 - val_loss: 0.0327 - val_accuracy: 0.9893\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.98780 to 0.98930, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 116s 62ms/step - loss: 0.0776 - accuracy: 0.9755 - val_loss: 0.0311 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.98930 to 0.98990, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 115s 61ms/step - loss: 0.0724 - accuracy: 0.9771 - val_loss: 0.0301 - val_accuracy: 0.9899\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.98990\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 115s 61ms/step - loss: 0.0651 - accuracy: 0.9801 - val_loss: 0.0255 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.98990 to 0.99180, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 870s 464ms/step - loss: 0.0646 - accuracy: 0.9805 - val_loss: 0.0240 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.99180\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 0.0537 - accuracy: 0.9823 - val_loss: 0.0230 - val_accuracy: 0.9920\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.99180 to 0.99200, saving model to D:\\Ananya\\IIIT--D\\HandWritingRecognition\\trainPart1\\model2_3.h5\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 0.0544 - accuracy: 0.9835 - val_loss: 0.0277 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.99200\n"
     ]
    }
   ],
   "source": [
    "history2 = model2_3.fit(x_train,y_train,batch_size=32,epochs=10, callbacks=callbacks_list ,verbose=1 , validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###comparing the models \n",
    "# model2_3 : trained from scratch \n",
    "# model2_22 : using pretrained model \n",
    "\n",
    "\n",
    "\n",
    "### comparing the convergence time \n",
    "\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['model2_22', 'model2_3'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['model2_22', 'model2_3'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model trained from the scratch is: 0.9827333092689514\n",
      "Accuracy of the model trained using pretrained model is: 0.9833833575248718\n"
     ]
    }
   ],
   "source": [
    "#final accuracy\n",
    "print(\"Accuracy of the model trained from the scratch is: \" + str(history1.history['accuracy'][-1]))\n",
    "print(\"Accuracy of the model trained using pretrained model is: \" + str(history2.history['accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the model trained from the scratch is: 0.9827333092689514\n",
      "Validation accuracy of the model trained using pretrained model is: 0.9833833575248718\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation accuracy of the model trained from the scratch is: \" + str(history1.history['accuracy'][-1]))\n",
    "print(\"Validation accuracy of the model trained using pretrained model is: \" + str(history2.history['accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion Matrix for model trained from scratch\n",
    "(x_train, y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "img_row = x_test[0].shape[0]\n",
    "img_col = x_test[0].shape[1]\n",
    "\n",
    "x_test  = np.array(x_test)\n",
    "\n",
    "x_test=x_test.reshape(x_test.shape[0], img_row, img_col, 1)\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 132ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anany\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 837us/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 945us/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_class = [] #contains the predicted value \n",
    "y_ = y_test[:20]\n",
    "#y_ contains the true value\n",
    "for img in x_test[:20]:\n",
    "    img = img.reshape(1,28,28,1)\n",
    "    pred_class.append(model2_3.predict_classes(img, verbose=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_scratch=confusion_matrix(y_, pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "### Confusion Matrix for model trained using pre-trained model \n",
    "\n",
    "pred_class = [] #contains the predicted value \n",
    "y_ = y_test[:20]\n",
    "#y_ contains the true value\n",
    "for img in x_test[:20]:\n",
    "    img = img.reshape(1,28,28,1)\n",
    "    predict_prob = model2_22.predict(img, verbose=1)\n",
    "    predict_classes = np.argmax(predict_prob,axis=1)\n",
    "    pred_class.append(predict_classes)\n",
    "    #pred_class.append(model2_22.predict_classes(img, verbose=1))\n",
    "    \n",
    "    \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_pretrained=confusion_matrix(y_,pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for the model trained from scratch is : \n",
      "[[3 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The confusion matrix for the model trained from scratch is : \")\n",
    "print(cm_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for the model trained from pretrained model is : \n",
      "[[3 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 3 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The confusion matrix for the model trained from pretrained model is : \")\n",
    "print(cm_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
